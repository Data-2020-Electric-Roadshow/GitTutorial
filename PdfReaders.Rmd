---
title: "PDF Readers"
author: "David Buch"
date: "5/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Franco

#### FPC Electric Power Statistics Scraping

1. One method I found that seems applicable to the electric power statistics files is using the tabulizer and tidyverse packages in R in conjunction with pdftools to scrape data from these files. The tabulizer package allows us to pull specific tables from the pdf, based on their indexes, which would be useful in this case as the files contain two tables for each state/region in the country: one describing total generating capacity, and another describing total electricity production. So using tabulizer, we can scrape all of the capacity tables or production tables at once, and create separate files for each of these, each with information on every state, or we can create separate files for each state, depending on our approach. The tidyverse and pdftools packages allow us to manipulate the tables afer they have been scraped, if we want to change variable names or something like that. I also found a stackoverflow post that shows how you can filter specific parts of the tables to include in the output file. That post is linked [here](https://stackoverflow.com/questions/54403728/tabulizer-package-in-r-how-to-scrape-tables-after-specific-title). This could be useful because each of the tables in these files is subdivided based on the source from which the energy  was derived. If we are interested in investigating the energy mix and looking at how it changes over time, it would be important to include this specific data on how much electricity came from each source each year. There's more useful R documentation [here](https://cran.r-project.org/web/packages/tabulizer/tabulizer.pdf).  

2. Another method that could be useful for scraping data from these pdf files is using the tabula-py or camelot-py python package/library. This is a very similar alternative to the option described previously using R. [This](https://www.thepythoncode.com/article/extract-pdf-tables-in-python-camelot) post walks through an example using the camelot library and it seems to do a great job of converting the pdf table to a csv. This also seems particularly geared towards our purposes of data scraping and should allow us to preserve the same formatting of the tables as in the pdfs. This is because in cases where there are multiple column or row labels stacked on top of each other, the program recognizes this and creates empty space to keep the table properly aligned. One issue is that each table has to be exported to individual csv files, but a work around is that we can tell python to compress the csvs and it will produce a zip file of the csvs for all of the tables in the pdf. The tabula library is also promising and could simplify the scraping process as it can automatically convert the extracted table to a csv directly rather than having an intermediary step of creating data frames, however, this may not be possible if the tables are not accurately extracted, or if we want to modify their format, etc.  


## Tri
1. `pdftools` is a great package for pdf scraping. The package contains the 
`pdf_text` function, which renders all textboxes on a text canvas and returns a 
character vector of equal length to the number of pages in the PDF file. It also
has the `pdf_data` function, which is more low level and returns one data frame 
per page, containing one row for each textbox in the PDF.
I am currently translating some pdf into csv file using this package. It is not
very good at detecting small texts or small details, but it translates some of 
the clearer files into csv files. I think this is useful as the first step 
because we can see some initial results. 

`tabulizer` is another great package for pdf scraping. This package provides 
R bindings to the Tabula java library, which can be used to computationally 
extract tables from PDF documents. The main function extract_tables() mimics the 
command-line behavior of the Tabula, by extracting all tables from a PDF file 
and, by default, returns those tables as a list of character matrices in R.
I am having some troubles setting up the `rJava`, `tabulizer`, and 
`tabulizerjars` package since I have not implemented Java in my system. I have
not testes this package. 

Update: I was able to translate two pages of 1 report to csv file using 
`pdftools`. However, I have not found any other way to extract data more 
efficient. I have updated these changes in the Git Repo. 

2. `tabula-py` is a Python based package to convert pdf file into csv
file. You can read tables from PDF and convert into pandas's DataFrame. 
tabula-py also enables you to convert a PDF file into CSV/TSV/JSON file. I have
some troubles implementing this in VSCode. I have not tried in any other 
platforms (Anaconda)


## ZhiHao
1. Python package `tabula_convert_into` (I recommend)

a. description: 
tabula-py is a simple Python wrapper of
tabula-java, which can read table of PDF.
You can read tables from PDF and convert into pandasâ€™s DataFrame. 
tabula-py also enables you to convert a PDF file into
CSV/TSV/JSON file. 
I think we can focus on its convert function. (If
familiar with python, the tabula_read_pdf function can be helpful)

b. Try:
I test this command using the table on page 23 of
"~/From FPC/1942.FPC.Production of Electric Energy.pdf".
This works well, and returns a relatively easy-handling
csv file. 

c. Problems on csv file:
- Some records (numbers) are splitted into two different
rows. Some people say adding "lattice=True" can solve
this problem, but it returns me an empty file. Maybe we
can fix this later by data transformation?
- Scanned page is not clear enough to get the correct
record. For example, "242,940" is parsed as "242, 9 sy".
- Same format table on different page of original pdf
file returns different table format in csv file. For
example, I also tried to convert page 19 of the test
file. It has the same format, but in csv file, it is
somewhat diffenet. This can make it difficult for later
iteration.


2. R package `pdftools` + `pdftables`

a. description:
An unofficial R package to convert PDF tables to format more amenable to analysis (like csv). It's easy to combine these two commands

b. problem:
It requires API key for automatically scraping pdf
files. The maximum page we can scrape is 50....If we
want to scrape more, we have to pay for $40 for 5000
pages. I think the quality of conversion does not deserve $40 for our scanned pdf.

Note: if interested, you can see the code and generated csv file in directory "test_csv_zhihao". If you want to try the R code, you can use my API key in the script.
