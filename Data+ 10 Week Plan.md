# 10-Week Plan

## Expectations

* 40 hours/week of honest effort. With lunch breaks, etc. probably just over 30 hours/week of focused work. *Nothing more and nothing less.* More and you will get burnt out and quality of work will suffer. Less and you will not get very much out of the program.
* In the best case this summer you will develop three key skills, none of which are related to data science
  1. Independently-guided work
  2. Collaborative-work
  3. "researcher's resilience"
* I will be available, especially during our morning Stand-Ups and weekly Meetings, to answer your questions and provide guidance where possible, but I expect you to get your hand dirty searching the internet (stack exchage, wikipedia, scientific literature, etc.) for answers. Believe it or not, sifting through help articles and staying on track is a skill you will get better at.

## Recommendation

* Get to know one another as much as possible, you each have unique technical and non-technical strengths. Communicaton will be a tremendous barrier this summer and to avoid awkward dependency conflicts you need to maintain contact. Your teammates can either be your greatest assests or an obstacle.
* Prioritize getting things "working" in small steps, then worry about making it pretty. (that said, once you have working code *do* make sure it is well documented for later). Modularize analysis where possible. This is good for short term productivity and sanity, and in the long term greatly reduces time spent debugging, in my experience.

## Task 1 (Weeks 1-3): Data Processing

* Week 1: By the end of the week, will be able to transfer *something* from a pdf to an excel sheet/csv file. **Daily progress updates via git**
  * Day 1: Install command line tools, get connected to all data sources and tools, learn scrum etc. Meet the team. What is the project about?
  * Day 2: Confer about presentation, assign file type for each team member to try to scrape. Research pdf scraping tools **git merge info about 2 pdf readers to a common document**
  * Day 3: Present to me in stand up very briefly about the 2 pdf readers you will try to use and why. Spend the day doing that.
  * Day 4: Did anyone have success on day 3? Work together so data from all pdf file types is being thrown into csv files, one way or another. Meeting with companion Story+ team.
* Week 2: **Migrate to Notion project planning** and organize Epics/sprints in weekly meeting. Document what information is present in the data. Begin coarse planning of analysis and automate better scraping/cleaning of data.
* Week 3: Continue to develop scraping/cleaning data pipeline. At the weekly meeting, decide what needs to be done to reach target: **Clean, well-formed data by the end of the week**. Documentation of what was done to scrape and clean all the data. Presentation on Friday.

## Task 2 (Weeks 4-7): Analysis 

* In light of cleaned data, refactor plan for analysis
* Research statistical techniques and justify their use
* Analyze data. Answer stakeholder's questions of interest in key graphics and, if possible, statistical inferences. Quantify uncertainty.
* Prepare for second plenary talk to larger Data+ community

## Task 3 (Weeks 8-10): Extensions

* Are inferences from task 2 applicable to developing electricity markets in the 21st century?
* Visit the smithsonian!!

